#@meta {desc: 'training (only) configuration', date: '2024-02-01'}

[import]
sections = list: lp_imp

[lp_imp]
config_file = example/train/lp.conf

[amr_trainer_default]
trainer_type = generate_t5wtense
model_name = lp_${trainer_type}
trainer_is_generator = True

[amr_default]
parse_model = t5

[amr_generate_t5wtense_trainer]
#installer = instance: amr_${amr_trainer_default:trainer_type}_installer

# uncomment the next two lines to train from the last amrlib checkpoint (error
# loading checkpoint tokenizer if commented out)
pretrained_path_or_model = t5-base
training_config_file = resource(zensols.amr): resources/train/model_generate_xfm_t5_base_wTT.json
training_config_overrides = dict: {'hf_args': {'num_train_epochs': 1}}
